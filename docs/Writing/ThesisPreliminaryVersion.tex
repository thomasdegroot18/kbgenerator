\documentclass{article}
\usepackage{graphicx, color}
\usepackage[a4paper,margin=2cm]{geometry}
\usepackage{url}


\newcommand{\red}[1]{{\color{red}{#1}}}

\begin{document}

	\begin{titlepage}
	
	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here
	\center % Center everything on the page
	
	%----------------------------------------------------------------------------------------
	%	HEADING SECTIONS
	%----------------------------------------------------------------------------------------
	
	%\includegraphics[width=\linewidth]{uvaENG}\\[2.5cm]
	\textsc{\Large MSc Artificial Intelligence}\\[0.2cm]
	\textsc{\normalsize Track: \red{track}}\\[1.0cm] % track
	\textsc{\Large Master Thesis}\\[0.5cm] 
	
	%----------------------------------------------------------------------------------------
	%	TITLE SECTION
	%----------------------------------------------------------------------------------------
	
	\HRule \\[0.4cm]
	{ \huge \bfseries \red{Designing custom inconsistent knowledge graphs}}\\[0.4cm] % Title of your document
	\HRule \\[0.5cm]
	
	%----------------------------------------------------------------------------------------
	%	AUTHOR SECTION
	%----------------------------------------------------------------------------------------
	
	by\\[0.2cm]
	\textsc{\Large \red{Thomas de Groot}}\\[0.2cm] %your name
	\red{student number}\\[1cm]
	
	
	%----------------------------------------------------------------------------------------
	%	DATE SECTION
	%----------------------------------------------------------------------------------------
	
	{\Large \today}\\[1cm] % Date, change the \today to a set date if you want to be precise
	
	\red{Number of Credits}\\ %
	\red{Period in which the research was carried out}\\[1cm]%
	
	%----------------------------------------------------------------------------------------
	%	COMMITTEE SECTION
	%----------------------------------------------------------------------------------------
	\begin{minipage}[t]{0.4\textwidth}
		\begin{flushleft} \large
			\emph{Supervisor:} \\
			\red{Dr A \textsc{Person} }% Supervisor's Name
		\end{flushleft}
	\end{minipage}
	~
	\begin{minipage}[t]{0.4\textwidth}
		\begin{flushright} \large
			\emph{Assessor:} \\
			\red{Dr A  \textsc{Person}}\\
		\end{flushright}
	\end{minipage}\\[2cm]
	
	%----------------------------------------------------------------------------------------
	%	LOGO SECTION
	%----------------------------------------------------------------------------------------
	
	\framebox{\rule{0pt}{2.5cm}\rule{2.5cm}{0pt}}\\[0.5cm]
	%\includegraphics[width=2.5cm]{figure}\\ % Include a department/university logo - this will require the graphicx package
	\textsc{\large \red{institute name}}\\[1.0cm] % 
	
	%----------------------------------------------------------------------------------------
	
	\vfill % Fill the rest of the page with whitespace
	
\end{titlepage}
\pagenumbering{roman}
% Writing of the report

\section*{Definitions}


\newpage
\section*{Acknowledgments}



\newpage
\section*{Abstract}
The development of larger knowledge based systems is growing rapidly. and the reasoners over these large datasets are following quickly behind. While reasoning over these large knowledge graphs is improving ++ADD IN CITATION++, it is mandatory that these knowledge systems are consistent. With one inconsistency the knowledge graph can break and it is no longer possible to reason of these graphs. Several methods exists that clean the knowledge bases from these inconsistencies. Other methods try to reason around the inconsistencies or use other methods to incorporate the knowledge in their reasoners. While most methods work well, the test cases that are used for these models are not a great representation of the complete world wide web of linked data. Most of the test cases are selected for their characteristics, or the datasets are specifically designed for the test purposes of the method.\\
To improve the general availability of inconsistent knowledge bases we designed an general knowledge base generator that uses generalized forms of inconsistencies found in the LOD-a-LOT ++ADD IN CITATION++ and use these inconsistencies to build an inconsistent knowledge base that is designed according to a set of parameters that can be given by the user.\\


\newpage
\tableofcontents
\newpage
\pagenumbering{arabic}

\section{Introduction}
> Introduction
Error cleaning, linking instances and consistency checking are large topics researched in the field, but each researcher has his own choice of ontology on which they want to apply these new techniques that are used to 

> Why is this needed?
To illustrate the need of a good method that can provide a inconsistent knowledge base that matches a set of constraints and is easy to use. But that is not only it. Would also be possible to mimic other larger knowledge graphs that could be to big for algorithms to test on and make these knowledge bases smaller such that they can fit the algorithms without losing a lot of similarity with the larger knowledge base. 

> What are others doing or is already done?


> What are the advantages of my method?

> How is this method achieved?

> How is this paper written?\\
The first chapter explains the work that this paper is based on, as well as shows the work that others have done before us that is related or could benefit from the work we have done. The second chapter goes into the preliminaries, what is needed to even start the process what for preliminary work has been done to get the method working. The Approach and method explain more in detail how the inconsistent knowledge graphs are generated and how they are build according to some sets of standards. The experiments showcase the possible applications of this knowledge base generator and the possibility to generate smaller subsets of existing knowledge bases that do still match the existing knowledge bases but are only smaller in comparison. In the results the generated models are compared on bases of a set of criteria to be added in and the speed of the algorithm for retrieving these smaller knowledge bases. We conclude with a conclusion and discussion about the use cases, with an extension of future work.

\newpage
\section{Related Work}
>What is the state of the art?\\
While there is a large community on trying to build "consistent" knowledge bases on data that already exists. Or merging knowledge bases with possible conflicting data or triples, no found work is done on actively building and designing real inconsistent knowledge bases. It is therefor not possible to match this problem to any of the state of the art solutions as there are none. 
There is work done on inconsistency locating: \url{https://github.com/raki123/inconsistent-subgraph-query}\\
A generator for databases: UIS Database Generator\\
Sphinx generator \url{http://www.speech.cs.cmu.edu/tools/lmtool-new.html}
Random SAT generator

> What papers is this research build on?
While this subject is relatively new, the research in this subject connects to a multitude of other papers on which parts of this paper are loosely based, as well as that several test cases come forth out of these papers. \cite{Michael FÃ¤rber} shows the necessity of a knowledge base generator. In his paper a set of the largest Linked Datasets are compared on a large set of different criteria and show a framework for given a set of criteria which dataset would most closely resemble your expectations. While this is a good way to select your knowledge base for your experiments, this paper takes a step further and is would use the input of the user to generate a database that would closely resemble the larger database while giving the user the freedom to choose the amount of triples and the types of inconsistency.\\
\cite{Heiko Paulheim} showcases the need for a standardized evaluation method. In the survey the paper the research is based on shows that the papers choose sometimes different knowledge graph(s). This makes it harder to compare. Giving the user a hard time figuring out which research paper produces a good result in comparison with a different paper. Removing the this discrepancy would benefit the community as now it will be easier to compare algorithms.
This paper uses a 


\newpage
\section{Preliminaries}
Before we go deeper into the problem Approach and the method used it is necessary to first understand a set of key concepts that will be used in the coming chapters, although a general knowledge of mathematics is needed. it is also necessary to know where the data comes from.

\subsection{Knowledge}
>What is a knowledge graph?\\
A knowledge graph is a "graphical" representation of a set of statements with information, the knowledge base. This can be anything about everything. The data is stored in triple format. These are statements that consist out of <Subject> <Predicate> <Object>. Where the Subject and Object are vertices in the graph and the Predicate is the edge between the subject and the object. Both the vertices and the edges have different properties. Even though the size of the graph can change according to the amount of triples it has, the graph is connected and thus, it is possible to connect between all the vertices in the graph. 

>what is RDF(S) OWL?\\
RDF(s)

>What are Inconsistencies?\\
Inconsistencies are mistakes in the knowledge graph. It is possible to have data that contradicts other data. An example is a >>><<< . In this example the error is in that two different parts of the data contradict each other. While this can be true in real life data and happen her the real world. Knowledge bases have a hard time understanding inconsistencies and the moment a inconsistency in the data is found it is no longer possible to reason over the data as this means that we can no longer assume that any part of the data is "correct", while it may perfectly be that case.

\subsection{Data}
To provide a good overview about natural occurring data on the web the largest possible dataset full with linked data would be the best starting point. It would not only help with finding naturally occurring errors, but would also help solve any scaling issues that could have occurred with by first experimenting on smaller datasets. The data we are talking about is of course the LOD-a-lot.

\subsubsection{LOD-a-lot}
The LOD-a-lot >>CITATION<< is the largest? open source linked data set that is readily available. The data is stored in a HDT. Where an HDT is a (Header, Dictionary, Triples) File. In this way the data is structured in a compact manner with a binary serialization format for RDF. Which has the advantage that querying large files is fast due to optimizations done which have been tailored on reading large data set. >>CITATION<<

>What is Linked Open data?\\
Linked Open data >>CITATION<< is coined by Tim Berners Lee and is used to describe data that fills the five stars that is set by him. 
...\\
...\\
...\\
This the data is stored in such a way that it is open for everyone. Linked to existing sources. And it can be used as open source.

\newpage
\section{Approach \& Method}
This chapter will explain in detail how the implementation of the model has been done. Explaining how the inconsistencies were retrieved from the LOD-a-lot, and generalized. Then the chapter goes more in detail about how the generalized inconsistencies are used in the experiments. 

\subsection{Retrieval of the inconsistent patterns}
>Why are the subgraphs needed?\\

A reasoner is limited in the amount of data it can reason over within limited time. As more data means many different facts which will take more time to reason over. Even with optimizations it is not possible to reason over the LOD-a-lot without using massive amounts of CPU or taking a very long time. To improve the time needed to reason over we can either add more resources or make the amount of data to reason over smaller. The first option is not viable, but the second option is. 
It does however mean that we need to generate correct subgraphs without losing any of the inconsistencies that could be found only in the complete graph.

>So how are the subgraphs generated?\\

Generating the subgraphs without losing information is tricky as links are broken to retrieve a small part of the graph from a large graph. The goal is to minimize the loss by making sure that the severed links will be added in later stages of the pass through the complete graph.

To retrieve subgraphs from the graph we use rootnodes. Generating rootnodes is done by taking a triple from the complete graph as our starting point. Then for this triple the subject is chosen as the root node. This is the node from which the subgraph is generated. To build the subgraph the node expanded upon such that a graph will be generated around the rootnode. 
The rootnode is expanded by recursively taking all the triples for which the rootnode is the subject, then if the amount of triples that has been added stays under a limit all the objects are added to the list and are now used as root nodes. These will then be questioned and expanded upon with the rootnode, expanding the graph quickly if there are many links and more slowly with less links. After doing this until a stopping criteria is reached. A limit on the amount of triples or no more links can be found. Due only going into one direction namely the switching to the object position the amount of links is quite often limited. 
It is also expected that most to all inconsistencies will be found as the limit can be set to an acceptable amount and with the expectation that all links will be visited the amount of inconsistencies will be almost complete. We can not however say that this will find all inconsistencies as it would be possible to design an inconsistency that is larger than the limit set. >>But I Expect that this will not happen.

So how are the inconsistencies retrieved from the subgraphs?\\
Now that the large knowledge graph has been split into multiple smaller subgraphs we can start to locate the inconsistencies that coincide in these subgraphs. This is done by employing an reasoner that can locate inconsistencies, the importance is though that this reasoner does not reason over triples and adds in new triples that the reasoner could implicitly add. This way we do not find inconsistencies that could occur in the dataset without the user knowing, or that the reasoner the inconsistency "shrinks" due to the reasoner being able to shortcut inconsistencies, as seen in the EXAMPLE.

Each subgraph can have multiple inconsistencies, so each subgraph is checked extensively for inconsistencies.  >>TO AVOID DUPLICATES: << Each inconsistency in handled seperately.


\subsection{Generalizing the patterns on basis of isomorphic graphs}
Why is generalization needed?\\
All inconsistencies are different, but the inconsistencies can be in essence one and the same inconsistency. As the amount of inconsistencies rise to more than a >massive number<, it would be more manageable if we shrunk the list of different inconsistencies down by finding generalization between the inconsistencies, as multiple inconsistencies can be equal when we remove the specific classes and instances hanging on the vertices. This could lower the amount of inconsistencies while it is still possible to retrieve these specific inconsistencies with simple SPARQL queries that can be run over the graph. 

So how are the inconsistencies generalized?\\
The first step is figuring out how it is possible to generalize the patterns without losing information about the inconsistencies or at least a minimal amount. The first step can be started with the notion of isomorphic graphs. Isomorphic graphs are a great way to generalize graphs without losing information that is irreversibly lost. But as the information on the vertices is not relevant for the inconsistencies the information in on the edges is important. As the connections between the nodes describe the possibility to be inconsistent as described in the >EXAMPLE<. While de edges are also important for the isomorphic graphs. As two graphs could be isomorphic according to normal standards, with the notion that the edges also must align graphs could now be no longer isomorphic. See >EXAMPLE<

The problem however is that to check if two graphs are isomorphic is difficult to test >CITATION<. 
To improve the efficiency of the isomorphic matching the amount of graphs are pruned such that matching the graphs will not happen with graphs that can in no way be isomorphic. There are a number of characteristic before a graph is isomorphic:\\
 - It needs to have the same number of vertices.\\
 - It needs to have the same number of edges.\\
 - It needs to have the same amount of degrees.\\
 - In our case it also needs to have the same amount of edges based on the edge types we have.\\
 
If two graphs do match we then run the matching algorithm and only if the matching algorithm returns true the two graphs are matched.
The algorithm that is used in matching is explained here >ALGORITHM<


\subsection{Generalizing the patterns on basis of ...}
>>>>>>>>>.................................<<<<<<<<<<


\subsection{Locating the most used inconsistency cases}
Finished with the generalization is it now possible to be able to retrieve meaningful statistics about the generalized inconsistencies. Interesting statistics are the top 10 most common mistakes:


Extra ADDITIONS TO THE APPROACH:

\begin{enumerate}
	\item Import the LOD-a-lot
	\item Retrieve a sub graph of the LOD cloud by choosing a vertex as root node and retrieving a small part of LOD-a-lot by expanding the root node.
	\item Check the sub graph for inconsistencies. 
	\item Retrieve the inconsistencies and check if they can be generalized.
	\item Generalize on basis of isomorphic, check if vertex isomorphic and edge isomorphic
	\item Add in ... steps to further generalize.
	\item Count the amount of occurrences per "generalized" inconsistent subgraph
	\item Make a generator that uses input from the generalized inconsistencies and user given parameters to build a knowledge base.
	\item The generator uses a .... ALGORITHM to build these graphs from the LOD-a-lot Cloud.
	\item The generator returns the graph to the user.
	\item Testing the generated graphs with the method to check if the consistencies are correct. 
	\item Implement several methods to test whether these methods perform as well as that the researchers propose.
	\item Show results.
\end{enumerate}


\newpage
\section{Experiments}
Possible Experiments for testing:
https://openproceedings.org/2018/conf/edbt/paper-61.pdf
Tools for Finding Inconsistencies in Real-world Logic-based Systems


\newpage
\section{Results}


\newpage
\section{Conclusion}
>What conclusions can be found based upon?


\subsection{Future Work}
> What are the next steps for the algorithm?

\newpage
\section{Bibliography}


\newpage
\section{Appendices}
\subsection{Appendix A}

\end{document}


	

