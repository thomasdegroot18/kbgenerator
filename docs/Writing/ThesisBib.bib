@article{HeikoP:2016,
	author = {Paulheim, Heiko},
	year = {2016},
	month = {12},
	pages = {489-508},
	title = {Knowledge graph refinement: A survey of approaches and evaluation methods},
	volume = {8},
	journal = {Semantic Web},
	doi = {10.3233/SW-160218}
}

@article{MichaelF:2017,
	author = {Färber, Michael and Bartscherer, Frederic and Menne, Carsten and Rettinger, Achim},
	year = {2017},
	month = {03},
	pages = {1-53},
	title = {Linked data quality of DBpedia, Freebase, OpenCyc, Wikidata, and YAGO},
	volume = {9},
	journal = {Semantic Web},
	doi = {10.3233/SW-170275}
}

@inproceedings{JavierD:2017,
	title={LOD-a-lot - A Queryable Dump of the LOD Cloud},
	author={Javier D. Fern{\'a}ndez and Wouter Beek and Miguel A. Mart{\'i}nez-Prieto and Mario Arias},
	booktitle={International Semantic Web Conference},
	year={2017}
}

@InProceedings{WouterB:2014,
	author={"Beek, Wouter
	and Rietveld, Laurens
	and Bazoobandi, Hamid R.
	and Wielemaker, Jan
	and Schlobach, Stefan"},
	editor={"Mika, Peter
	and Tudorache, Tania
	and Bernstein, Abraham
	and Welty, Chris
	and Knoblock, Craig
	and Vrande{\v{c}}i{\'{c}}, Denny
	and Groth, Paul
	and Noy, Natasha
	and Janowicz, Krzysztof
	and Goble, Carole"},
	title={"LOD Laundromat: A Uniform Way of Publishing Other People's Dirty Data"},
	booktitle={"The Semantic Web -- ISWC 2014"},
	year={"2014"},
	publisher={"Springer International Publishing"},
	address={"Cham"},
	pages={"213--228"},
	abstract={"It is widely accepted that proper data publishing is difficult. The majority of Linked Open Data (LOD) does not meet even a core set of data publishing guidelines. Moreover, datasets that are clean at creation, can get stains over time. As a result, the LOD cloud now contains a high level of dirty data that is difficult for humans to clean and for machines to process."},
	isbn={"978-3-319-11964-9"}
}

@article{JavierD:2013,
	author = {Javier D. Fernández and Miguel A. Martínez-Prieto and Claudio Gutiérrez and Axel Polleres and Mario Arias},
	title = {Binary RDF Representation for Publication and Exchange (HDT)},
	journal = {Web Semantics: Science, Services and Agents on the World Wide Web},
	volume = {19},
	number = {0},
	year = {2013},
	publisher = {Elsevier},
	keywords = {RDF, Binary formats, Data compaction and compression, RDF metrics},
	abstract = {The current Web of Data is producing increasingly large RDF datasets. Massive publication efforts of RDF data driven by initiatives like the Linked Open Data movement, and the need to exchange large datasets has unveiled the drawbacks of traditional RDF representations, inspired and designed by a document-centric and human-readableWeb. Among the main problems are high levels of verbosity/redundancy and weak machine-processable capabilities in thedescription of these datasets. This scenario calls for efficient formats for publication and exchange.This article presents a binary RDF representation addressing these issues. Based on a set ofmetrics that characterizes the skewed structure of real-world RDF data, we develop a proposal of an RDF representation thatmodularly partitions and efficiently represents three components of RDF datasets: Header information, a Dictionary, and the actual Triples structure (thus called HDT). Our experimental evaluation shows that datasets in HDT format can be compacted by more than fifteen times as compared to current naive representations, improving both parsing and processing while keeping a consistent publication scheme. Specific compression techniques over HDT further improve these compression rates and prove to outperform existing compression solutions for efficient RDF exchange.},
	issn = {1570-8268},
	url = {http://www.websemanticsjournal.org/index.php/ps/article/view/328}
}


@online{rdfPrimer:2014,
	author = "G. Schreiber, Y. Raimond",
	title = "RDF primer 1.1",
	date = "24-06-2014",
	organization = "W3C Working Group",
	note = "\url{https://www.w3.org/TR/rdf11-primer/}",
}

@online{SPARQLPrimer:2013,
	author = "S. Harris, A. Seaborne",
	title = "SPARQL 1.1 Query Language",
	date = "21-03-2013",
	organization = "W3C Working Group",
	note = "\url{https://www.w3.org/TR/sparql11-query/}",
}

@online{OWLPrimer:2012,
	author="P. Hitzler and M. Krötzsch and B. Parsia and P. Patel-Schneider and S. Rudolph"
	title="OWL 2 Web Ontology Language Primer (Second Edition)"
	date="11-12-2012"
	note= "\url{https://www.w3.org/TR/owl-primer/}"
	organization="W3C Working Group",
}

@InProceedings{MatthewH:2009,
	author="Horridge, Matthew
	and Parsia, Bijan
	and Sattler, Ulrike",
	editor="Godo, Llu{\'i}s
	and Pugliese, Andrea",
	title="Explaining Inconsistencies in OWL Ontologies",
	booktitle="Scalable Uncertainty Management",
	year="2009",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="124--137",
	abstract="Justifications play a central role as the basis for explaining entailments in OWL ontologies. While techniques for computing justifications for entailments in consistent ontologies are theoretically and practically well-understood, little is known about the practicalities of computing justifications for inconsistent ontologies. This is despite the fact that justifications are important for repairing inconsistent ontologies, and can be used as a basis for paraconsistent reasoning. This paper presents algorithms, optimisations, and experiments in this area. Surprisingly, it turns out that justifications for inconsistent ontologies are more ``difficult'' to compute and are often more ``numerous'' than justifications for entailments in consistent ontologies: whereas it is always possible to compute some justifications, it is often not possible to compute all justifications for real world inconsistent ontologies.",
	isbn="978-3-642-04388-8"
}

@article{ChristianB:2009,
	title={Linked Data - The Story So Far},
	author={Christian Bizer and Tom Heath and Tim Berners-Lee},
	journal={Int. J. Semantic Web Inf. Syst.},
	year={2009},
	volume={5},
	pages={1-22}
}


@ARTICLE{LCordella2004,
	author={L. P. Cordella and P. Foggia and C. Sansone and M. Vento},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	title={A (sub)graph isomorphism algorithm for matching large graphs},
	year={2004},
	volume={26},
	number={10},
	pages={1367-1372},
	keywords={computational complexity;pattern matching;graph theory;graph isomorphism algorithm;graphs matching;subgraph isomorphism;spatial complexity;technical drawings;Pattern recognition;Pattern matching;Pattern analysis;Application software;NP-complete problem;Performance analysis;Algorithm design and analysis;Testing;Performance evaluation;Relational databases;Index Terms- Graph-subgraph isomorphism;large graphs;attributed relational graphs.;Algorithms;Artificial Intelligence;Cluster Analysis;Computer Graphics;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Information Storage and Retrieval;Numerical Analysis, Computer-Assisted;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted;Subtraction Technique},
	doi={10.1109/TPAMI.2004.75},
	ISSN={0162-8828},
	month={Oct},}