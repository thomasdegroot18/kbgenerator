\documentclass{article}
\usepackage{graphicx, color}
\usepackage[a4paper,margin=2cm]{geometry}
\usepackage{url}


\newcommand{\red}[1]{{\color{red}{#1}}}

\begin{document}

	\begin{titlepage}
	
	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here
	\center % Center everything on the page
	
	%----------------------------------------------------------------------------------------
	%	HEADING SECTIONS
	%----------------------------------------------------------------------------------------
	
	%\includegraphics[width=\linewidth]{uvaENG}\\[2.5cm]
	\textsc{\Large MSc Artificial Intelligence}\\[0.2cm]
	\textsc{\normalsize Track: \red{track}}\\[1.0cm] % track
	\textsc{\Large Master Thesis}\\[0.5cm] 
	
	%----------------------------------------------------------------------------------------
	%	TITLE SECTION
	%----------------------------------------------------------------------------------------
	
	\HRule \\[0.4cm]
	{ \huge \bfseries \red{Designing custom knowledge bases\\ For inconsistency work}}\\[0.4cm] % Title of your document
	\HRule \\[0.5cm]
	
	%----------------------------------------------------------------------------------------
	%	AUTHOR SECTION
	%----------------------------------------------------------------------------------------
	
	by\\[0.2cm]
	\textsc{\Large \red{Thomas de Groot}}\\[0.2cm] %your name
	\red{student number}\\[1cm]
	
	
	%----------------------------------------------------------------------------------------
	%	DATE SECTION
	%----------------------------------------------------------------------------------------
	
	{\Large \today}\\[1cm] % Date, change the \today to a set date if you want to be precise
	
	\red{Number of Credits}\\ %
	\red{Period in which the research was carried out}\\[1cm]%
	
	%----------------------------------------------------------------------------------------
	%	COMMITTEE SECTION
	%----------------------------------------------------------------------------------------
	\begin{minipage}[t]{0.4\textwidth}
		\begin{flushleft} \large
			\emph{Supervisor:} \\
			\red{Dr A \textsc{Person} }% Supervisor's Name
		\end{flushleft}
	\end{minipage}
	~
	\begin{minipage}[t]{0.4\textwidth}
		\begin{flushright} \large
			\emph{Assessor:} \\
			\red{Dr A  \textsc{Person}}\\
		\end{flushright}
	\end{minipage}\\[2cm]
	
	%----------------------------------------------------------------------------------------
	%	LOGO SECTION
	%----------------------------------------------------------------------------------------
	
	\framebox{\rule{0pt}{2.5cm}\rule{2.5cm}{0pt}}\\[0.5cm]
	%\includegraphics[width=2.5cm]{figure}\\ % Include a department/university logo - this will require the graphicx package
	\textsc{\large \red{institute name}}\\[1.0cm] % 
	
	%----------------------------------------------------------------------------------------
	
	\vfill % Fill the rest of the page with whitespace
	
\end{titlepage}
\pagenumbering{roman}
% Writing of the report

\section*{Definitions}


\newpage
\section*{Acknowledgments}



\newpage
\section*{Abstract}
The development of larger knowledge based systems is growing rapidly. and the reasoners over these large datasets are following quickly behind. While reasoning over these large knowledge graphs is improving ++ADD IN CITATION++, it is mandatory that these knowledge systems are consistent. With one inconsistency the knowledge graph can break and it is no longer possible to reason of these graphs. Several methods exists that clean the knowledge bases from these inconsistencies. Other methods try to reason around the inconsistencies or use other methods to incorporate the knowledge in their reasoners. While most methods work well, the test cases that are used for these models are not a great representation of the complete world wide web of linked data. Most of the test cases are selected for their characteristics, or the datasets are specifically designed for the test purposes of the method.\\
To improve the general availability of inconsistent knowledge bases we designed an general knowledge base generator that uses generalized forms of inconsistencies found in the LOD-a-LOT ++ADD IN CITATION++ and use these inconsistencies to build an inconsistent knowledge base that is designed according to a set of parameters that can be given by the user.\\


\newpage
\tableofcontents
\newpage
\pagenumbering{arabic}

\section{Introduction}


\newpage
\section{Related Work}
>What is the state of the art?\\

>What is the state of the art in inconsistency cleaning/removal?\\

>What are the preliminary work this paper is build on?\\


\url{http://www.semantic-web-journal.net/system/files/swj1366.pdf }
This paper shows the work of ... In the paper the most common used linked datasets are compared on a set of criteria. And a framework for comparing knowledge bases.\\

\url{https://link.springer.com/content/pdf/10.1007/978-3-642-23863-5_54.pdf}The representation of inconsistent knowledge in Advanced knowledge based systems
\url{https://link.springer.com/content/pdf/10.1007%2F978-3-319-13704-9.pdf}Inconsistency monitoring in large scientific knowledge bases pages 87-95
\url{https://openproceedings.org/2018/conf/edbt/paper-61.pdf}User-guided repairing of inconsistent knowledge bases
\url{https://reader.elsevier.com/reader/sd/pii/S0004370218300523?token=CE73B0A9A46B681E239DBA5DDC88F2507D9B38328CC6707AAEE38C3669BBE495B7068DD5DB2DD3FD7B9FB75F1A9D7A81}Measuring inconsistency with constraints for propositional knowledge bases
\url{https://dl.acm.org/citation.cfm?id=2145494} Random walk inference and learning in large scale knowledge bases
\url{http://www.aclweb.org/anthology/D14-1044} Incorporating vector space similarity in random walk inference over knowledge bases
\url{http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/download/3659/3898} Learning structured embeddings of knowledge bases
\url{https://ieeexplore.ieee.org/abstract/document/506705/} Current approaches to handling imperfect information in data and knowledge bases
\url{https://www.sciencedirect.com/science/article/pii/S0888613X14001029} Computational approaches to finding and measuring inconsistency in arbitrary knowledge bases
\url{https://arxiv.org/ftp/arxiv/papers/1205/1205.2613.pdf} Measuring inconsistency in probabilistic knowledge bases
\url{https://www.hindawi.com/journals/mpe/2015/134950/} A Max-Term Counting Based Knowledge Inconsistency Checking Strategy and Inconsistency Measure Calculation of Fuzzy Knowledge Based Systems
\url{http://vldb.org/conf/1999/P60.pdf} Extracting large-scale knowledge bases from the web


\newpage
\section{Preliminaries}
Before we go deeper into the problem Approach and the method used it is necessary to first understand a set of key concepts that will be used in the coming chapters, although a general knowledge of mathematics is needed. it is also necessary to know where the data comes from.

\subsection{Knowledge}
>What is a knowledge graph?\\
A knowledge graph is a "graphical" representation of a set of statements with information, the knowledge base. This can be anything about everything. The data is stored in triple format. These are statements that consist out of <Subject> <Predicate> <Object>. Where the Subject and Object are vertices in the graph and the Predicate is the edge between the subject and the object. Both the vertices and the edges have different properties. Even though the size of the graph can change according to the amount of triples it has, the graph is connected and thus, it is possible to connect between all the vertices in the graph. 

>what is RDF(S) OWL?\\
RDF(s)

>What are Inconsistencies?\\
Inconsistencies are mistakes in the knowledge graph. It is possible to have data that contradicts other data. An example is a >>><<< . In this example the error is in that two different parts of the data contradict each other. While this can be true in real life data and happen her the real world. Knowledge bases have a hard time understanding inconsistencies and the moment a inconsistency in the data is found it is no longer possible to reason over the data as this means that we can no longer assume that any part of the data is "correct", while it may perfectly be that case.

\subsection{Data}
To provide a good overview about natural occurring data on the web the largest possible dataset full with linked data would be the best starting point. It would not only help with finding naturally occurring errors, but would also help solve any scaling issues that could have occurred with by first experimenting on smaller datasets. The data we are talking about is of course the LOD-a-lot.

\subsubsection{LOD-a-lot}
The LOD-a-lot >>CITATION<< is the largest? open source linked data set that is readily available. The data is stored in a HDT. Where an HDT is a (Header, Dictionary, Triples) File. In this way the data is structured in a compact manner with a binary serialization format for RDF. Which has the advantage that querying large files is fast due to optimizations done which have been tailored on reading large data set. >>CITATION<<

>What is Linked Open data?\\
Linked Open data >>CITATION<< is coined by Tim Berners Lee and is used to describe data that fills the five stars that is set by him. 
...\\
...\\
...\\
This the data is stored in such a way that it is open for everyone. Linked to existing sources. And it can be used as open source.

\newpage
\section{Approach \& Method}
This chapter will explain in detail how the implementation of the model has been done. Explaining how the inconsistencies were retrieved from the LOD-a-lot, and generalized. Then the chapter goes more in detail about how the generalized inconsistencies are used in the experiments. 

\subsection{Retrieval of the inconsistent patterns}
>Why are the subgraphs needed?\\

A reasoner is limited in the amount of data it can reason over within limited time. As more data means many different facts which will take more time to reason over. Even with optimizations it is not possible to reason over the LOD-a-lot without using massive amounts of CPU or taking a very long time. To improve the time needed to reason over we can either add more resources or make the amount of data to reason over smaller. The first option is not viable, but the second option is. 
It does however mean that we need to generate correct subgraphs without losing any of the inconsistencies that could be found only in the complete graph.

>So how are the subgraphs generated?\\

Generating the subgraphs without losing information is tricky as links are broken to retrieve a small part of the graph from a large graph. The goal is to minimize the loss by making sure that the severed links will be added in later stages of the pass through the complete graph.

To retrieve subgraphs from the graph we use rootnodes. Generating rootnodes is done by taking a triple from the complete graph as our starting point. Then for this triple the subject is chosen as the root node. This is the node from which the subgraph is generated. To build the subgraph the node expanded upon such that a graph will be generated around the rootnode. 
The rootnode is expanded by recursively taking all the triples for which the rootnode is the subject, then if the amount of triples that has been added stays under a limit all the objects are added to the list and are now used as root nodes. These will then be questioned and expanded upon with the rootnode, expanding the graph quickly if there are many links and more slowly with less links. After doing this until a stopping criteria is reached. A limit on the amount of triples or no more links can be found. Due only going into one direction namely the switching to the object position the amount of links is quite often limited. 
It is also expected that most to all inconsistencies will be found as the limit can be set to an acceptable amount and with the expectation that all links will be visited the amount of inconsistencies will be almost complete. We can not however say that this will find all inconsistencies as it would be possible to design an inconsistency that is larger than the limit set. >>But I Expect that this will not happen.

So how are the inconsistencies retrieved from the subgraphs?\\
Now that the large knowledge graph has been split into multiple smaller subgraphs we can start to locate the inconsistencies that coincide in these subgraphs. This is done by employing an reasoner that can locate inconsistencies, the importance is though that this reasoner does not reason over triples and adds in new triples that the reasoner could implicitly add. This way we do not find inconsistencies that could occur in the dataset without the user knowing, or that the reasoner the inconsistency "shrinks" due to the reasoner being able to shortcut inconsistencies, as seen in the EXAMPLE.

Each subgraph can have multiple inconsistencies, so each subgraph is checked extensively for inconsistencies.  >>TO AVOID DUPLICATES: << Each inconsistency in handled seperately.


\subsection{Generalizing the patterns on basis of isomorphic graphs}
Why is generalization needed?\\
All inconsistencies are different, but the inconsistencies can be in essence one and the same inconsistency. As the amount of inconsistencies rise to more than a >massive number<, it would be more manageable if we shrunk the list of different inconsistencies down by finding generalization between the inconsistencies, as multiple inconsistencies can be equal when we remove the specific classes and instances hanging on the vertices. This could lower the amount of inconsistencies while it is still possible to retrieve these specific inconsistencies with simple SPARQL queries that can be run over the graph. 

So how are the inconsistencies generalized?\\
The first step is figuring out how it is possible to generalize the patterns without losing information about the inconsistencies or at least a minimal amount. The first step can be started with the notion of isomorphic graphs. Isomorphic graphs are a great way to generalize graphs without losing information that is irreversibly lost. But as the information on the vertices is not relevant for the inconsistencies the information in on the edges is important. As the connections between the nodes describe the possibility to be inconsistent as described in the >EXAMPLE<. While de edges are also important for the isomorphic graphs. As two graphs could be isomorphic according to normal standards, with the notion that the edges also must align graphs could now be no longer isomorphic. See >EXAMPLE<

The problem however is that to check if two graphs are isomorphic is difficult to test >CITATION<. 
To improve the efficiency of the isomorphic matching the amount of graphs are pruned such that matching the graphs will not happen with graphs that can in no way be isomorphic. There are a number of characteristic before a graph is isomorphic:\\
 - It needs to have the same number of vertices.\\
 - It needs to have the same number of edges.\\
 - It needs to have the same amount of degrees.\\
 - In our case it also needs to have the same amount of edges based on the edge types we have.\\
 
If two graphs do match we then run the matching algorithm and only if the matching algorithm returns true the two graphs are matched.
The algorithm that is used in matching is explained here >ALGORITHM<


\subsection{Generalizing the patterns on basis of ...}
>>>>>>>>>.................................<<<<<<<<<<


\subsection{Locating the most used inconsistency cases}
Finished with the generalization is it now possible to be able to retrieve meaningful statistics about the generalized inconsistencies. Interesting statistics are the top 10 most common mistakes:


Extra ADDITIONS TO THE APPROACH:

\begin{enumerate}
	\item Import the LOD-a-lot
	\item Retrieve a sub graph of the LOD cloud by choosing a vertex as root node and retrieving a small part of LOD-a-lot by expanding the root node.
	\item Check the sub graph for inconsistencies. 
	\item Retrieve the inconsistencies and check if they can be generalized.
	\item Generalize on basis of isomorphic, check if vertex isomorphic and edge isomorphic
	\item Add in ... steps to further generalize.
	\item Count the amount of occurrences per "generalized" inconsistent subgraph
	\item Make a generator that uses input from the generalized inconsistencies and user given parameters to build a knowledge base.
	\item The generator uses a .... ALGORITHM to build these graphs from the LOD-a-lot Cloud.
	\item The generator returns the graph to the user.
	\item Testing the generated graphs with the method to check if the consistencies are correct. 
	\item Implement several methods to test whether these methods perform as well as that the researchers propose.
	\item Show results.
\end{enumerate}


\newpage
\section{Experiments}
Possible Experiments for testing:
https://openproceedings.org/2018/conf/edbt/paper-61.pdf
Tools for Finding Inconsistencies in Real-world Logic-based Systems


\newpage
\section{Results}


\newpage
\section{Conclusion}
>What conclusions can be found based upon?


\section{Future Work}

\newpage
\section{Bibliography}


\newpage
\section{Appendices}
\subsection{Appendix A}

\end{document}


	

